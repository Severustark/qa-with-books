# app.py
# Tek dosyada RAG: ingest (PDF/TXT -> Chroma) + Gradio arayÃ¼zÃ¼ + OpenAI fallback
# KullanÄ±m:
#   1) python app.py --ingest   # data/docs iÃ§indeki PDF/TXT'leri indeksle
#   2) python app.py --run      # arayÃ¼zÃ¼ baÅŸlat

import os
import glob
import argparse
from typing import List, Tuple, Optional

from dotenv import load_dotenv


# ---- Ortam DeÄŸiÅŸkenleri / Ayarlar ----
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
CHAT_MODEL     = os.getenv("CHAT_MODEL", "gpt-4o-mini")            # dilediÄŸin modeli yaz
EMBED_MODEL    = os.getenv("EMBED_MODEL", "text-embedding-3-small")
DOCS_DIR       = os.getenv("DOCS_DIR", "data/docs")
VDB_DOCS       = os.getenv("VDB_DOCS", "vectordb/chroma_docs")
TOP_K          = int(os.getenv("TOP_K", "3"))
SIM_THRESHOLD  = float(os.getenv("SIM_THRESHOLD", "0.75"))         # eÅŸik altÄ± â†’ fallback

# ---- LangChain bileÅŸenleri ----
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ---- (Opsiyonel) UI ----
import gradio as gr


# =========================
#    Belge YÃ¼kleme & Ingest
# =========================
def load_all_documents(root: str):
    """
    DOCS_DIR altÄ±nda PDF ve TXT dosyalarÄ±nÄ± toplayÄ±p LangChain Document listesi dÃ¶ner.
    """
    files = glob.glob(os.path.join(root, "**", "*"), recursive=True)
    docs = []
    for f in files:
        lf = f.lower()
        if lf.endswith(".pdf"):
            docs.extend(PyPDFLoader(f).load())
        elif lf.endswith(".txt"):
            # TÃ¼rkÃ§e karakterler iÃ§in encoding veriyoruz
            docs.extend(TextLoader(f, encoding="utf-8").load())
    return docs


def ingest_docs() -> int:
    """
    PDF/TXT -> chunk -> embedding -> Chroma persist
    """
    if not OPENAI_API_KEY:
        raise RuntimeError("OPENAI_API_KEY bulunamadÄ±. .env dosyanÄ± veya ortam deÄŸiÅŸkenlerini kontrol et.")

    os.makedirs(VDB_DOCS, exist_ok=True)

    documents = load_all_documents(DOCS_DIR)
    if not documents:
        print(f"âš ï¸  '{DOCS_DIR}' iÃ§inde PDF/TXT bulunamadÄ±. Ã–nce dosya yerleÅŸtirip tekrar deneyin.")
        return 0

    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=150)
    chunks = splitter.split_documents(documents)

    embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY, model=EMBED_MODEL)
    vectordb = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=VDB_DOCS)
    vectordb.persist()

    print(f"âœ… {len(chunks)} parÃ§a indekslendi â†’ {VDB_DOCS}")
    return len(chunks)


# =========================
#        Soru-Cevap
# =========================
class DocsRAG:
    def __init__(self):
        if not OPENAI_API_KEY:
            raise RuntimeError("OPENAI_API_KEY bulunamadÄ±. .env dosyanÄ± veya ortam deÄŸiÅŸkenlerini kontrol et.")
        self.emb = OpenAIEmbeddings(api_key=OPENAI_API_KEY, model=EMBED_MODEL)
        self.db  = Chroma(persist_directory=VDB_DOCS, embedding_function=self.emb)
        self.llm = ChatOpenAI(api_key=OPENAI_API_KEY, model=CHAT_MODEL, temperature=0.2)

        # Kitapla ilgi filtresi iÃ§in anahtarlar
        self._book_keywords = [
            "kitap", "roman", "hikaye", "Ã¶ykÃ¼", "yazar", "karakter", "tema", "konu",
            "alÄ±ntÄ±", "sayfa", "bÃ¶lÃ¼m",
            "boÅŸ dolaplar", "bos dolaplar", "annie ernaux", "ernaux"
        ]
        # AnahtarlarÄ± normalize edilmiÅŸ halde sakla (karÅŸÄ±laÅŸtÄ±rma kolay olsun)
        self._norm_book_keywords = [self._normalize_tr(k) for k in self._book_keywords]

    @staticmethod
    def _normalize_tr(text: str) -> str:
        """TÃ¼rkÃ§e karakterleri sadeleÅŸtir + lower (Ã¶rn: 'BoÅŸ' -> 'bos')."""
        table = str.maketrans("Ã§ÄŸÄ±Ã¶ÅŸÃ¼Ã‡ÄÄ°Ã–ÅÃœ", "cgiosuCGIOSU")
        return text.translate(table).lower()

    def _is_book_related(self, question: str) -> bool:
        qn = self._normalize_tr(question)
        return any(k in qn for k in self._norm_book_keywords)

    def _search(self, question: str, k: int = TOP_K):
        return self.db.similarity_search_with_relevance_scores(question, k=k)

    def _format_prompt(self, question: str, context: Optional[str] = None) -> str:
        if context:
            return f"BaÄŸlam:\n{context}\n\nSoru: {question}\n\nCevap:"
        return f"Soru: {question}\n\nCevap:"

    def answer(self, question: str) -> Tuple[str, str]:
        """
        AkÄ±ÅŸ:
        1) Soru kitapla ilgili mi? DeÄŸilse yanÄ±t verme.
        2) Belgelerde semantik arama.
        3) En iyi skor eÅŸik Ã¼zerindeyse baÄŸlamla yanÄ±t.
        4) EÅŸik altÄ±ysa 'bilgi yok'.
        """
        # 1) Kitap filtresi
        if not self._is_book_related(question):
            return "Bu soru kitap/kitap iÃ§eriÄŸi ile ilgili gÃ¶rÃ¼nmÃ¼yor, yanÄ±t veremem.", "Filtre: kitap dÄ±ÅŸÄ±"

        # 2) Arama
        results = self._search(question, k=TOP_K)
        if not results:
            return "ÃœzgÃ¼nÃ¼m, belgelerde bu soruya dair bir bilgi bulamadÄ±m.", "Kaynak: Yok"

        top_score = results[0][1]

        # 3) EÅŸik kontrolÃ¼
        if top_score >= SIM_THRESHOLD:
            context = "\n\n".join([r[0].page_content for r in results])
            prompt  = self._format_prompt(question, context)
            try:
                out = self.llm.invoke(prompt).content
            except Exception:
                # Ola ki LLM Ã§aÄŸrÄ±sÄ±nda sorun olursa, en alakalÄ± parÃ§ayÄ± dÃ¶k
                out = results[0][0].page_content
            return out, f"Kaynak: Belgeler (benzerlik={top_score:.2f})"

        # 4) EÅŸik altÄ±
        return "ÃœzgÃ¼nÃ¼m, belgelerde bu soruya dair bir bilgi bulamadÄ±m.", f"Kaynak: EÅŸik altÄ± (en iyi={top_score:.2f})"


# =========================
#          UI
# =========================
def launch_ui():
    rag = DocsRAG()

    def ask(q):
        if not q or not q.strip():
            return "LÃ¼tfen bir soru yazÄ±n.", ""
        ans, src = rag.answer(q.strip())
        return ans, f"_{src}_"

    with gr.Blocks(title="ğŸ“š QA with Books (RAG)") as demo:
        gr.Markdown("## ğŸ“š QA with Books (RAG)\nPDF/TXT'ten cevap; bulunamazsa OpenAIâ€™a dÃ¼ÅŸer.")
        q = gr.Textbox(label="Soru", lines=2, placeholder="Ã–rn: Bu kitabÄ±n ana temasÄ± nedir?")
        a = gr.Markdown()
        s = gr.Markdown()
        gr.Button("Sor").click(ask, inputs=q, outputs=[a, s])
    demo.launch()


# =========================
#        CLI / main
# =========================
def main():
    parser = argparse.ArgumentParser(description="Tek dosyalÄ±k RAG: ingest + run")
    parser.add_argument("--ingest", action="store_true", help="DOCS_DIR iÃ§indeki PDF/TXT'leri Chroma'ya indeksle")
    parser.add_argument("--run",    action="store_true", help="Gradio arayÃ¼zÃ¼nÃ¼ baÅŸlat")
    args = parser.parse_args()

    # VarsayÄ±lan klasÃ¶rleri oluÅŸtur
    os.makedirs(DOCS_DIR, exist_ok=True)
    os.makedirs(VDB_DOCS, exist_ok=True)

    if not (args.ingest or args.run):
        parser.print_help()
        print("\nğŸ” Ã–rnek:")
        print("  python app.py --ingest   # Ã¶nce indeks oluÅŸtur")
        print("  python app.py --run      # sonra arayÃ¼zÃ¼ baÅŸlat")
        return

    if args.ingest:
        ingest_docs()

    if args.run:
        launch_ui()


if __name__ == "__main__":
    main()
